# SOME DESCRIPTIVE TITLE.
# Copyright (C) 1999-2020, The Alibaba Group Holding Ltd.
# This file is distributed under the same license as the mars package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2020.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: mars 0.4.0rc1\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2020-04-29 18:27+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"

#: ../../source/learn/generated/mars.learn.decomposition.PCA.rst:2
msgid "mars.learn.decomposition.PCA"
msgstr ""

#: mars.learn.decomposition.PCA:1 of
msgid "Principal component analysis (PCA)"
msgstr ""

#: mars.learn.decomposition.PCA:3 of
msgid ""
"Linear dimensionality reduction using Singular Value Decomposition of the"
" data to project it to a lower dimensional space. The input data is "
"centered but not scaled for each feature before applying the SVD."
msgstr ""

#: mars.learn.decomposition.PCA:7 of
msgid ""
"It uses the LAPACK implementation of the full SVD or a randomized "
"truncated SVD by the method of Halko et al. 2009, depending on the shape "
"of the input data and the number of components to extract."
msgstr ""

#: mars.learn.decomposition.PCA:11 of
msgid ""
"It can also use the scipy.sparse.linalg ARPACK implementation of the "
"truncated SVD."
msgstr ""

#: mars.learn.decomposition.PCA:14 of
msgid ""
"Notice that this class does not support sparse input. See "
":class:`TruncatedSVD` for an alternative with sparse data."
msgstr ""

#: mars.learn.decomposition.PCA:17 of
msgid "Read more in the :ref:`User Guide <PCA>`."
msgstr ""

#: mars.learn.decomposition.PCA of
msgid "Parameters"
msgstr ""

#: mars.learn.decomposition.PCA:19 of
msgid ""
"Number of components to keep. if n_components is not set all components "
"are kept::      n_components == min(n_samples, n_features)  If "
"``n_components == 'mle'`` and ``svd_solver == 'full'``, Minka's MLE is "
"used to guess the dimension. Use of ``n_components == 'mle'`` will "
"interpret ``svd_solver == 'auto'`` as ``svd_solver == 'full'``.  If ``0 <"
" n_components < 1`` and ``svd_solver == 'full'``, select the number of "
"components such that the amount of variance that needs to be explained is"
" greater than the percentage specified by n_components.  If ``svd_solver "
"== 'arpack'``, the number of components must be strictly less than the "
"minimum of n_features and n_samples.  Hence, the None case results in::"
"      n_components == min(n_samples, n_features) - 1"
msgstr ""

#: mars.learn.decomposition.PCA:19 of
msgid ""
"Number of components to keep. if n_components is not set all components "
"are kept::"
msgstr ""

#: mars.learn.decomposition.PCA:24 of
msgid ""
"If ``n_components == 'mle'`` and ``svd_solver == 'full'``, Minka's MLE is"
" used to guess the dimension. Use of ``n_components == 'mle'`` will "
"interpret ``svd_solver == 'auto'`` as ``svd_solver == 'full'``."
msgstr ""

#: mars.learn.decomposition.PCA:28 of
msgid ""
"If ``0 < n_components < 1`` and ``svd_solver == 'full'``, select the "
"number of components such that the amount of variance that needs to be "
"explained is greater than the percentage specified by n_components."
msgstr ""

#: mars.learn.decomposition.PCA:32 of
msgid ""
"If ``svd_solver == 'arpack'``, the number of components must be strictly "
"less than the minimum of n_features and n_samples."
msgstr ""

#: mars.learn.decomposition.PCA:35 of
msgid "Hence, the None case results in::"
msgstr ""

#: mars.learn.decomposition.PCA:39 of
msgid ""
"If False, data passed to fit are overwritten and running "
"fit(X).transform(X) will not yield the expected results, use "
"fit_transform(X) instead."
msgstr ""

#: mars.learn.decomposition.PCA:43 of
msgid ""
"When True (False by default) the `components_` vectors are multiplied by "
"the square root of n_samples and then divided by the singular values to "
"ensure uncorrelated outputs with unit component-wise variances.  "
"Whitening will remove some information from the transformed signal (the "
"relative variance scales of the components) but can sometime improve the "
"predictive accuracy of the downstream estimators by making their data "
"respect some hard-wired assumptions."
msgstr ""

#: mars.learn.decomposition.PCA:43 of
msgid ""
"When True (False by default) the `components_` vectors are multiplied by "
"the square root of n_samples and then divided by the singular values to "
"ensure uncorrelated outputs with unit component-wise variances."
msgstr ""

#: mars.learn.decomposition.PCA:47 of
msgid ""
"Whitening will remove some information from the transformed signal (the "
"relative variance scales of the components) but can sometime improve the "
"predictive accuracy of the downstream estimators by making their data "
"respect some hard-wired assumptions."
msgstr ""

#: mars.learn.decomposition.PCA:52 of
#, python-format
msgid ""
"auto :     the solver is selected by a default policy based on `X.shape` "
"and     `n_components`: if the input data is larger than 500x500 and the"
"     number of components to extract is lower than 80% of the smallest"
"     dimension of the data, then the more efficient 'randomized'     "
"method is enabled. Otherwise the exact full SVD is computed and     "
"optionally truncated afterwards. full :     run exact full SVD calling "
"the standard LAPACK solver via     `scipy.linalg.svd` and select the "
"components by postprocessing arpack :     run SVD truncated to "
"n_components calling ARPACK solver via     `scipy.sparse.linalg.svds`. It"
" requires strictly     0 < n_components < min(X.shape) randomized :     "
"run randomized SVD by the method of Halko et al."
msgstr ""

#: mars.learn.decomposition.PCA:58 of
msgid "auto :"
msgstr ""

#: mars.learn.decomposition.PCA:54 of
#, python-format
msgid ""
"the solver is selected by a default policy based on `X.shape` and "
"`n_components`: if the input data is larger than 500x500 and the number "
"of components to extract is lower than 80% of the smallest dimension of "
"the data, then the more efficient 'randomized' method is enabled. "
"Otherwise the exact full SVD is computed and optionally truncated "
"afterwards."
msgstr ""

#: mars.learn.decomposition.PCA:61 of
msgid "full :"
msgstr ""

#: mars.learn.decomposition.PCA:61 of
msgid ""
"run exact full SVD calling the standard LAPACK solver via "
"`scipy.linalg.svd` and select the components by postprocessing"
msgstr ""

#: mars.learn.decomposition.PCA:65 of
msgid "arpack :"
msgstr ""

#: mars.learn.decomposition.PCA:64 of
msgid ""
"run SVD truncated to n_components calling ARPACK solver via "
"`scipy.sparse.linalg.svds`. It requires strictly 0 < n_components < "
"min(X.shape)"
msgstr ""

#: mars.learn.decomposition.PCA:67 of
msgid "randomized :"
msgstr ""

#: mars.learn.decomposition.PCA:68 of
msgid "run randomized SVD by the method of Halko et al."
msgstr ""

#: mars.learn.decomposition.PCA:70 of
msgid "Tolerance for singular values computed by svd_solver == 'arpack'."
msgstr ""

#: mars.learn.decomposition.PCA:72 of
msgid ""
"Number of iterations for the power method computed by svd_solver == "
"'randomized'."
msgstr ""

#: mars.learn.decomposition.PCA:75 of
msgid ""
"If int, random_state is the seed used by the random number generator; If "
"RandomState instance, random_state is the random number generator; If "
"None, the random number generator is the RandomState instance used by "
"`np.random`. Used when ``svd_solver`` == 'arpack' or 'randomized'."
msgstr ""

#: mars.learn.decomposition.PCA:83 of
msgid ""
"Principal axes in feature space, representing the directions of maximum "
"variance in the data. The components are sorted by "
"``explained_variance_``."
msgstr ""

#: mars.learn.decomposition.PCA of
msgid "type"
msgstr ""

#: mars.learn.decomposition.PCA:87 of
msgid "tensor, shape (n_components, n_features)"
msgstr ""

#: mars.learn.decomposition.PCA:91 of
msgid "The amount of variance explained by each of the selected components."
msgstr ""

#: mars.learn.decomposition.PCA:93 of
msgid "Equal to n_components largest eigenvalues of the covariance matrix of X."
msgstr ""

#: mars.learn.decomposition.PCA:96 mars.learn.decomposition.PCA:105
#: mars.learn.decomposition.PCA:113 of
msgid "tensor, shape (n_components,)"
msgstr ""

#: mars.learn.decomposition.PCA:100 of
msgid "Percentage of variance explained by each of the selected components."
msgstr ""

#: mars.learn.decomposition.PCA:102 of
msgid ""
"If ``n_components`` is not set then all components are stored and the sum"
" of the ratios is equal to 1.0."
msgstr ""

#: mars.learn.decomposition.PCA:109 of
msgid ""
"The singular values corresponding to each of the selected components. The"
" singular values are equal to the 2-norms of the ``n_components`` "
"variables in the lower-dimensional space."
msgstr ""

#: mars.learn.decomposition.PCA:117 of
msgid "Per-feature empirical mean, estimated from the training set."
msgstr ""

#: mars.learn.decomposition.PCA:119 of
msgid "Equal to `X.mean(axis=0)`."
msgstr ""

#: mars.learn.decomposition.PCA:121 of
msgid "tensor, shape (n_features,)"
msgstr ""

#: mars.learn.decomposition.PCA:125 of
msgid ""
"The estimated number of components. When n_components is set to 'mle' or "
"a number between 0 and 1 (with svd_solver == 'full') this number is "
"estimated from input data. Otherwise it equals the parameter "
"n_components, or the lesser value of n_features and n_samples if "
"n_components is None."
msgstr ""

#: mars.learn.decomposition.PCA:131 of
msgid "int"
msgstr ""

#: mars.learn.decomposition.PCA:135 of
msgid ""
"The estimated noise covariance following the Probabilistic PCA model from"
" Tipping and Bishop 1999. See \"Pattern Recognition and Machine "
"Learning\" by C. Bishop, 12.2.1 p. 574 or "
"http://www.miketipping.com/papers/met-mppca.pdf. It is required to "
"compute the estimated data covariance and score samples."
msgstr ""

#: mars.learn.decomposition.PCA:141 of
msgid ""
"Equal to the average of (min(n_features, n_samples) - n_components) "
"smallest eigenvalues of the covariance matrix of X."
msgstr ""

#: mars.learn.decomposition.PCA:144 of
msgid "float"
msgstr ""

#: mars.learn.decomposition.PCA:147 of
msgid "References"
msgstr ""

#: mars.learn.decomposition.PCA:148 of
msgid ""
"For n_components == 'mle', this class uses the method of *Minka, T. P. "
"\"Automatic choice of dimensionality for PCA\". In NIPS, pp. 598-604*"
msgstr ""

#: mars.learn.decomposition.PCA:151 of
msgid ""
"Implements the probabilistic PCA model from: Tipping, M. E., and Bishop, "
"C. M. (1999). \"Probabilistic principal component analysis\". Journal of "
"the Royal Statistical Society: Series B (Statistical Methodology), 61(3),"
" 611-622. via the score and score_samples methods. See "
"http://www.miketipping.com/papers/met-mppca.pdf"
msgstr ""

#: mars.learn.decomposition.PCA:158 of
msgid "For svd_solver == 'arpack', refer to `scipy.sparse.linalg.svds`."
msgstr ""

#: mars.learn.decomposition.PCA:160 of
msgid ""
"For svd_solver == 'randomized', see: *Halko, N., Martinsson, P. G., and "
"Tropp, J. A. (2011). \"Finding structure with randomness: Probabilistic "
"algorithms for constructing approximate matrix decompositions\". SIAM "
"review, 53(2), 217-288.* and also *Martinsson, P. G., Rokhlin, V., and "
"Tygert, M. (2011). \"A randomized algorithm for the decomposition of "
"matrices\". Applied and Computational Harmonic Analysis, 30(1), 47-68.*"
msgstr ""

#: mars.learn.decomposition.PCA:170 of
msgid "Examples"
msgstr ""

#: mars.learn.decomposition.PCA:192 of
msgid ""
":class:`KernelPCA`, :class:`SparsePCA`, :class:`TruncatedSVD`, "
":class:`IncrementalPCA`"
msgstr ""

#: mars.learn.decomposition.PCA.__init__:1 of
msgid "Initialize self.  See help(type(self)) for accurate signature."
msgstr ""

#: ../../source/learn/generated/mars.learn.decomposition.PCA.rst:13
msgid "Methods"
msgstr ""

#: ../../source/learn/generated/mars.learn.decomposition.PCA.rst:26:<autosummary>:1
msgid ""
":obj:`__init__ <mars.learn.decomposition.PCA.__init__>`\\ "
"\\(\\[n\\_components\\, copy\\, whiten\\, ...\\]\\)"
msgstr ""

#: ../../source/learn/generated/mars.learn.decomposition.PCA.rst:26:<autosummary>:1
msgid "Initialize self."
msgstr ""

#: ../../source/learn/generated/mars.learn.decomposition.PCA.rst:26:<autosummary>:1
msgid ""
":obj:`fit <mars.learn.decomposition.PCA.fit>`\\ \\(X\\[\\, y\\, "
"session\\, run\\_kwargs\\]\\)"
msgstr ""

#: ../../source/learn/generated/mars.learn.decomposition.PCA.rst:26:<autosummary>:1
msgid "Fit the model with X."
msgstr ""

#: ../../source/learn/generated/mars.learn.decomposition.PCA.rst:26:<autosummary>:1
msgid ""
":obj:`fit_transform <mars.learn.decomposition.PCA.fit_transform>`\\ "
"\\(X\\[\\, y\\, session\\]\\)"
msgstr ""

#: ../../source/learn/generated/mars.learn.decomposition.PCA.rst:26:<autosummary>:1
msgid "Fit the model with X and apply the dimensionality reduction on X."
msgstr ""

#: ../../source/learn/generated/mars.learn.decomposition.PCA.rst:26:<autosummary>:1
msgid ""
":obj:`get_covariance <mars.learn.decomposition.PCA.get_covariance>`\\ "
"\\(\\[session\\]\\)"
msgstr ""

#: ../../source/learn/generated/mars.learn.decomposition.PCA.rst:26:<autosummary>:1
msgid "Compute data covariance with the generative model."
msgstr ""

#: ../../source/learn/generated/mars.learn.decomposition.PCA.rst:26:<autosummary>:1
msgid ""
":obj:`get_params <mars.learn.decomposition.PCA.get_params>`\\ "
"\\(\\[deep\\]\\)"
msgstr ""

#: ../../source/learn/generated/mars.learn.decomposition.PCA.rst:26:<autosummary>:1
msgid "Get parameters for this estimator."
msgstr ""

#: ../../source/learn/generated/mars.learn.decomposition.PCA.rst:26:<autosummary>:1
msgid ""
":obj:`get_precision <mars.learn.decomposition.PCA.get_precision>`\\ "
"\\(\\[session\\]\\)"
msgstr ""

#: ../../source/learn/generated/mars.learn.decomposition.PCA.rst:26:<autosummary>:1
msgid "Compute data precision matrix with the generative model."
msgstr ""

#: ../../source/learn/generated/mars.learn.decomposition.PCA.rst:26:<autosummary>:1
msgid ""
":obj:`inverse_transform "
"<mars.learn.decomposition.PCA.inverse_transform>`\\ \\(X\\[\\, "
"session\\]\\)"
msgstr ""

#: ../../source/learn/generated/mars.learn.decomposition.PCA.rst:26:<autosummary>:1
msgid "Transform data back to its original space."
msgstr ""

#: ../../source/learn/generated/mars.learn.decomposition.PCA.rst:26:<autosummary>:1
msgid ""
":obj:`score <mars.learn.decomposition.PCA.score>`\\ \\(X\\[\\, y\\, "
"session\\]\\)"
msgstr ""

#: ../../source/learn/generated/mars.learn.decomposition.PCA.rst:26:<autosummary>:1
msgid "Return the average log-likelihood of all samples."
msgstr ""

#: ../../source/learn/generated/mars.learn.decomposition.PCA.rst:26:<autosummary>:1
msgid ""
":obj:`score_samples <mars.learn.decomposition.PCA.score_samples>`\\ "
"\\(X\\[\\, session\\]\\)"
msgstr ""

#: ../../source/learn/generated/mars.learn.decomposition.PCA.rst:26:<autosummary>:1
msgid "Return the log-likelihood of each sample."
msgstr ""

#: ../../source/learn/generated/mars.learn.decomposition.PCA.rst:26:<autosummary>:1
msgid ""
":obj:`set_params <mars.learn.decomposition.PCA.set_params>`\\ "
"\\(\\*\\*params\\)"
msgstr ""

#: ../../source/learn/generated/mars.learn.decomposition.PCA.rst:26:<autosummary>:1
msgid "Set the parameters of this estimator."
msgstr ""

#: ../../source/learn/generated/mars.learn.decomposition.PCA.rst:26:<autosummary>:1
msgid ""
":obj:`transform <mars.learn.decomposition.PCA.transform>`\\ \\(X\\[\\, "
"session\\]\\)"
msgstr ""

#: ../../source/learn/generated/mars.learn.decomposition.PCA.rst:26:<autosummary>:1
msgid "Apply dimensionality reduction to X."
msgstr ""

